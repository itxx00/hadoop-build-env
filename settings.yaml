# ==============================================================
# The document descripts deployment parameters that defines 
# which file is fundamental and how to install these necessary 
# components. 
#
# @author: Xiaoyang Sun (xshaun@outlook.com). 
# Welcome e-mails from consulting and communicating.
# --------------------------------------------------------------


# ==============================================================
# The following descripts parameters' definations and values.
# Users can customize each field as their own demands. 

# [$]
# It defines the mode of deploying hadoop cluster. 
# Optional values as following:
#   - standalone
#   - pseudo-distributed 
#   - fully-distributed
#
#mode: 'pseudo-distributed' 
mode: 'fully-distributed' 

# [$]
# It defines the folder's absolute path of sourcecode. 
# Empty or non-existing folder is fine bacasuse the stage of 
# 'clear' and 'download' can create it automatically.
# !!! Must ensure its correctness and avoid misoperations.!!!
#
sourcecode: '/opt/rose-source-on-hadoop/'

# [$]
# It defines the folder's absolute path of binary code. 
# Empty or non-existing folder is fine bacasuse the stage of 
# 'distribute' can create it automatically.
# !!! Must ensure its correctness and avoid misoperations.!!!
#
binarycode: '/opt/rose/'

# [$]
# Here descripts host, username and password of overall roles 
# including deamons in Yarn and HDFS and also a proxy that helps
# manage the cluster. 
#
# Abbreviation: 
#   controlp - control proxy (controller)
#   resourcem - reource manager (in Yarn)
#   nodem - node manager (in Yarn)
#   namen - name node (in HDFS)
#   datan - data node (in HDFS)
#
roles:
  'controlp': 
    'host': 'localhost'
    'usr': 'boy'
    'pwd': 'xboy'

  'resourcem':
    'hosts':
      - 'localhost' # primary
    'usr': 'boy'
    'pwd': 'xboy'

  'nodem':
    'hosts':
      - '192.168.106.71'
    'usr': 'ubuntu'  # 'root'
    'pwd': '123456'

  'namen':
    'hosts':
      - 'localhost' # primary 
      - 'localhost' # secondary
    'usr': 'ubuntu'
    'pwd': '123456'
    'dir': '/hdfs/nn' # <-*
    'sdir': '/hdfs/snn' # <-*

  'datan':
    'hosts':
      - 'localhost' 
    'usr': 'boy'
    'pwd': 'xboy'
    'dir': '/hdfs/dn' # <-*

# [$]
# GROUP & USER definition of operation and runtime
#
opt:
  group: 'hadoop'
  user: 'yarn'

# [$]
# Here defines concrete content/script(s) in each step that is
# minimum unit of scheduling.
# 
# Make 'scripts/' as root directory and define value format is 
# 'roles.function', such as 'controlp.clear_sourcecode'. The 
# point(.) means folder separator so users can customize 
# hierarchical structure to organize customized scripts, such as
# 'fruit.banana.eaten' means './scripts/fruit/banana/eaten.py'
#
steps:
  # install prerequisites used to compile hadoop
  'cpicp': 'controlp.install_compilation_prerequisites'
  # setup passphraseless
  'cpsp': 'controlp.setup_passphraseless'
  # remove all files under codepath folder
  'cpcc': 'controlp.clear_sourcecode'
  # download binary code into sourcecode
  'cpdbc': 'controlp.download_bin_code'
  # download source code into sourcecode
  'cpdsc': 'controlp.download_src_code'
  # compile source code under codepath folder
  'cpcsc': 'controlp.compile_src_code'
  # distribute binary package to nodes
  'cpdbp': 'controlp.distribute_binary_package'

  # prerequisites while running hadoop
  'cirp': 'common.install_runtime_prerequisites'
  # add users and group
  'caug': 'common.add_user_group'
  #change mode and own, workers
  'ccbmo': 'common.change_binarycode_mode_own'

# [$]
# Here defines concrete steps in each stage that is combination 
# of several processes/steps.
#
stages:
  'init': 
    - 'cpicp'
    - 'cpsp'
    - 'cpcc'
    - 'cpdsc'
    - 'cpcsc'
    - 'cpdbp'
    - 'cirp'
    - 'caug'
    - 'ccbmo'

  'initproxy':
    - 'cpicp'
    - 'cpsp'
    - 'cpcc'
    - 'cpdsc'
    
  'keyless':
    - 'cpsp'

  'clear':
    - 'cpcc'

  'compile':
    - 'cpcsc'

  'deloy':
    - 'cpdbp'

  'test' : 
    - 'cirp'
    - 'caug'
    - 'ccbmo'
