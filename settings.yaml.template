# ==============================================================
# The document descripts deployment parameters that defines 
# which file is fundamental and how to install these necessary 
# components. 
#
# WELCOME YOUR EMAILS ABOUT COMMUNICATION AND CONSULTATION 
# AUTHOR: XIAOYANG SUN (xshaun@outlook.com).ALL RIGHTS RESERVED
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
# --------------------------------------------------------------


# ==============================================================
# The following descripts parameters' definations and values.
# Users can customize each field as their own demands. 

# [$]
# It defines the version of source code, which affects the name of binary jars.
#
version: '3.0.0-beta1' 

# [$]
# It defines the folder's absolute path of sourcecode. 
# Empty or non-existing folder is fine bacasuse the stage of 
# 'clear' and 'download' can create it automatically.
# !!! Must ensure its correctness and avoid misoperations.!!!
#
controlp_base_path: '/opt/rose-source-on-hadoop/'

# [$]
# It defines the folder's absolute path of binary code. 
# Empty or non-existing folder is fine bacasuse the stage of 
# 'distribute' can create it automatically.
# !!! Must ensure its correctness and avoid misoperations.!!!
#
cluster_base_path: '/opt/rose/'

# [$]
# GROUP & USER definition for operation and runtime in cluster
#
opt:
  group: 'hadoop'
  user: 'yarn'

# [$]
# Here descripts host, username and password of overall roles 
# including deamons in Yarn and HDFS and also a proxy that helps
# manage the cluster. 
#
# Abbreviation: 
#   controlp - control proxy (controller)
#   resourcem - reource manager (in Yarn)
#   nodem - node manager (in Yarn)
#   namen - name node (in HDFS)
#   datan - data node (in HDFS)
#
tmpmaster: &tmpmaster
  hosts:
    - '192.168.7.81'

tmpslaves: &tmpslaves
  hosts:
    - '192.168.7.80'
    - '192.168.7.79'
    - '192.168.7.78'
    - '192.168.7.77'
    - '192.168.7.76'

roles:
  controlp: 
    'host': 'localhost'
    'usr': 'boy'
    'pwd': 'xboy'

  resourcem:
    <<: *tmpmaster
    # hosts+:
    #   - '?.?.?.?' # primary
    'usr': 'ivic'
    'pwd': 'cloudos$123'

  nodem:
    <<: *tmpslaves    
    # hosts+:
    #   -
    'usr': 'ivic'
    'pwd': 'cloudos$123'

  namen:
    <<: *tmpmaster
    # hosts+:  
    #   - '?.?.?.?' # primary 
    #   - '?.?.?.?' # secondary
    'usr': 'ivic'
    'pwd': 'cloudos$123'
    'dir': 'hdfs/nn' # <-*
    'sdir': 'hdfs/snn' # <-*

  datan:
    <<: *tmpslaves
    # hosts+:
    #   -
    'usr': 'ivic'
    'pwd': 'cloudos$123'
    'dir': 'hdfs/dn' # <-*

# [$]
# Here defines concrete content/script(s) in each step that is
# minimum unit of scheduling.
# 
# Make 'scripts/' as root directory and define value format is 
# 'roles.function', such as 'controlp.clear_sourcecode'. The 
# point(.) means folder separator so users can customize 
# hierarchical structure to organize customized scripts, such as
# 'fruit.banana.eaten' means './scripts/fruit/banana/eaten.py'
#
steps:
  # install prerequisites used to compile hadoop
  'cpicp': 'controlp.install_compilation_prerequisites'
  # setup passphraseless
  'cpsp': 'controlp.setup_passphraseless'
  # remove all files under codepath folder
  'cpcc': 'controlp.clear_sourcecode'
  # download binary code into sourcecode
  'cpdbc': 'controlp.download_bin_code'
  # download source code into sourcecode
  'cpdsc': 'controlp.download_src_code'
  # init compile source code under codepath folder (clean dependency ...)
  'cpicsc': 'controlp.init_compile_src_code'
  # compile source code under codepath folder
  'cpcsc': 'controlp.compile_src_code'
  # prepare for distributing
  'cpdbpp': 'controlp.distribute_binary_package_prep'
  # distribute binary package to nodes
  'cpdbp': 'controlp.distribute_binary_package'
  # configure *-site.xml
  'cpcs': 'controlp.configure_site'

  # prerequisites while running hadoop
  'cirp': 'common.install_runtime_prerequisites'
  # add users and group
  'caug': 'common.add_user_group'
  # change mode and own, workers
  'ccbmo': 'common.change_binarycode_mode_own'
  # start all
  'cstart': 'common.start'
  # stop all
  'cstop': 'common.stop'
  # clean
  'cclean': 'common.clean'

  # name-node formats file system
  'nnffs': 'namen.format_file_system'

# [$]
# Here defines concrete steps in each stage that is combination 
# of several processes/steps.
#
stages:
  'init': 
    - 'cpicp'
    - 'cpsp'
  # - 'cpcc'
  # - 'cpdsc'
    - 'cpicsc'
    - 'cpdbpp'
    - 'cpdbp'
    - 'cpcs'
    - 'cirp'
    - 'caug'
    - 'ccbmo'
    - 'nnffs'

  'initcontrolp':
    - 'cpicp'
    - 'cpsp'
  # - 'cpcc'
  # - 'cpdsc'
  # - 'cpicsc'

  'initcompile':
    - 'cpicsc'

  'initcluster':
    - 'cirp'
    - 'caug'
    - 'ccbmo'

  'initdeploy':
    - 'cpsp'
    - 'cpicsc'
    - 'cpdbpp'
    - 'cpdbp'
    - 'cpcs'
    - 'cirp'
    - 'caug'
    - 'ccbmo'
    - 'nnffs'
    - 'cstart'

  'deploy':
    - 'cpcsc'
    - 'cstop'
    - 'cpdbp'
    - 'cpcs'
    - 'caug'
    - 'ccbmo'
    - 'cstart'

  'compile':
    - 'cpcsc'

  'config':
    - 'cpcs'

  'syncp':
    - 'cpdbpp'

  'sync':
    - 'cpdbp'

  'start':
    - 'cstart'

  'stop':
    - 'cstop'

  'clean':
    - 'cclean'

  'step':
    - 'anyone'
